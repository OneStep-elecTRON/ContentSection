{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        " ! pip install -q kaggle"
      ],
      "metadata": {
        "id": "xVJvvvmLUmgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "o6PI6ddpUojH",
        "outputId": "2814d8c4-88a1-4ced-b9c0-29e13e40bef4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b782273c-5ff8-4d0c-9cb4-6f334c4ac24c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b782273c-5ff8-4d0c-9cb4-6f334c4ac24c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"prernamittal03\",\"key\":\"6f26e84e32a449f0630256a9659ff1d4\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXU-EJnsUzSa",
        "outputId": "8ce63348-25ec-43cb-9838-57f88ff3467d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "F7yJ_PeGU106"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ! kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t3nrvIWVRjJ",
        "outputId": "617dc44e-8a1e-4b3c-8339-a1acc9a7da51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                            title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "-------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "iamsouravbanerjee/world-population-dataset                     World Population Dataset                             17KB  2022-08-31 11:20:04          12459        358  1.0              \n",
            "whenamancodes/hr-employee-attrition                            Employee Analysis | Attrition Report                 50KB  2022-09-12 10:46:33           1079         33  1.0              \n",
            "pantanjali/unemployment-dataset                                Unemployment dataset                                 17KB  2022-09-08 08:26:10           4222        107  1.0              \n",
            "whenamancodes/student-performance                              Student Performance                                  18KB  2022-09-15 01:15:37           2048         49  1.0              \n",
            "whenamancodes/students-performance-in-exams                    Students Performance in Exams                         9KB  2022-09-14 15:14:54           2212         49  1.0              \n",
            "harshsingh2209/tesla-stock-pricing-20172022                    TESLA stock pricing (2017-2022)                      28KB  2022-09-18 14:56:29            936         29  1.0              \n",
            "thedevastator/airplane-crashes-and-fatalities                  Airplane Crashes and Fatalities                     582KB  2022-09-20 05:30:35           1359         52  0.9411765        \n",
            "ariyoomotade/netflix-data-cleaning-analysis-and-visualization  Netflix Data: Cleaning, Analysis and Visualization  270KB  2022-08-26 09:25:43           6348        146  1.0              \n",
            "alexandrepetit881234/korean-demographics-20002022              Korean demographics 2000-2022                       101KB  2022-09-15 11:59:31            509         28  1.0              \n",
            "whenamancodes/netflix-prime-video-disney-hulu                  Netflix Disney+ Prime Video Hulu Shows Collection   101KB  2022-09-13 09:05:20           1128         30  1.0              \n",
            "whenamancodes/alcohol-effects-on-study                         Alcohol Effects On Study                             18KB  2022-09-15 03:21:04           1402         52  1.0              \n",
            "thedevastator/mcdonalds-ice-cream-machines-broken-timeseries   McDonalds Ice Cream Machines Breaking - Timeseries  404KB  2022-09-14 23:51:09           1260         42  1.0              \n",
            "whenamancodes/violence-against-women-girls                     Violence Against Women & Girls                       88KB  2022-09-12 08:46:49           1007         42  1.0              \n",
            "deepcontractor/smoke-detection-dataset                         Smoke Detection Dataset                               2MB  2022-08-21 06:29:34           3851        106  1.0              \n",
            "thedevastator/weather-prediction                               Weather Prediction                                  936KB  2022-09-06 12:07:29            875         32  0.9705882        \n",
            "cashncarry/fifa-23-complete-player-dataset                     FIFA 23 Complete Player Dataset [UPD:29/09/22]        2MB  2022-09-29 19:34:27            606         29  0.88235295       \n",
            "whenamancodes/data-science-fields-salary-categorization        Data Science Fields Salary Categorization             7KB  2022-09-10 07:53:45           1277         43  1.0              \n",
            "moazzimalibhatti/co2-emission-by-countries-year-wise-17502022  CO2 Emission by countries Year wise (1750-2022)     280KB  2022-09-14 07:43:00           1061         55  1.0              \n",
            "sergylog/ab-test-data                                          A/B test data                                        28KB  2022-09-16 17:29:06            628        100  1.0              \n",
            "advaypatil/youtube-statistics                                  Youtube Statistics                                    2MB  2022-08-26 02:03:19           2818         76  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c 'Quora-Insincere-Questions-Classification'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbTIODmQVRlY",
        "outputId": "ff497a18-ae72-4c82-b145-cb53088dd3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quora-Insincere-Questions-Classification.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir datakaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1WWDLeGVRnu",
        "outputId": "e5c85785-dbc0-4be9-f735-08733add52c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘datakaggle’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip Quora-Insincere-Questions-Classification.zip -d datakaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G01KzXQBWbNv",
        "outputId": "306866a0-3fe6-4075-f373-453571d73e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  Quora-Insincere-Questions-Classification.zip\n",
            "replace datakaggle/embeddings.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace datakaggle/sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace datakaggle/test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace datakaggle/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Naive Bayes Algorithm</b></h1><hr>\n",
        "<p>In this notebook, we will understand a probabilistic machine learning algorithm known as Naive Bayes algorithm and all essential concepts related to it.\n",
        "\n",
        "Naive Bayes is based on Bayes’s theorem which gives an assumption of independence among predictors. This classifier assumes that the presence of a particular feature in a class is not related to the presence of any other variable. Even if these features are related to each other, a Naive Bayes classifier would consider all of these properties independently when calculating the probability of a particular outcome.\n",
        "\n",
        "A Naive Bayesian model is easy to build and useful for massive datasets. It's simple and is known to outperform even highly sophisticated classification methods.</p>\n",
        "\n",
        "<b>Pros:</b><br>\n",
        "<li>This algorithm works very fast.\n",
        "<li>It can also be used to solve multi-class prediction problems as it’s quite useful with them.\n",
        "<li>This classifier performs better than other models with less training data if the assumption of independence of features holds.\n",
        "\n",
        "<b>Cons:</b><br>\n",
        "<li>It assumes that all the features are independent which means anyone can hardly find a set of independent features.\n",
        "\n"
      ],
      "metadata": {
        "id": "WNCu6ouZCVDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Sample 1:</h2>\n"
      ],
      "metadata": {
        "id": "DaY0SOwoIFRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=142)\n",
        "Naive_Bayes = GaussianNB()\n",
        "Naive_Bayes.fit(X_train, y_train)\n",
        "prediction_results = Naive_Bayes.predict(X_test)  \n",
        "print(prediction_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAHIaxFWIMyk",
        "outputId": "d6638f4c-a7a5-4d73-83d9-53276fe2ee06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 2 1 1 0 0 2 1 1 1 2 0 1 0 2 1 1 2 2 1 0 1 2 1 2 2 0 1 2 1 2 1 2 2 1\n",
            " 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Sample 2: Step-By-Step</h2>\n",
        "The goal is to develop a Naïve Bayes classification model that identifies and flags insincere questions. The dataset can be downloaded from <a href = \"https://www.kaggle.com/c/quora-insincere-questions-classification/data\">here</a>. Once you have downloaded the train and test data, load it and check."
      ],
      "metadata": {
        "id": "QIb-GOZlI0a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing dependecies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "#importing data\n",
        "train = pd.read_csv('datakaggle/train.csv')\n",
        "print(train.head())\n",
        "test = pd.read_csv('datakaggle/test.csv')"
      ],
      "metadata": {
        "id": "l9yxrtSHfgJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bc24f1-f8cb-4805-e233-2e2305be8e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                    qid                                      question_text  \\\n",
            "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
            "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
            "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
            "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
            "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
            "\n",
            "   target  \n",
            "0       0  \n",
            "1       0  \n",
            "2       0  \n",
            "3       0  \n",
            "4       0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Text Preprocessing</h2><p>\n",
        " \n",
        "The next step is to preprocess text before splitting the dataset into a train and test set. The preprocessing steps involve: Removing Numbers, Removing Punctuations in a string, Removing Stop Words, Stemming of Words and Lemmatization of Words.</p>\n",
        "\n",
        "<h2>Constructing a Naive Bayes Classifier</h2><p>\n",
        " \n",
        "Combine all the preprocessing techniques and create a dictionary of words and each word’s count in training data.\n",
        "\n",
        "Calculate probability for each word in a text and filter the words which have a probability less than threshold probability. Words with probability less than threshold probability are irrelevant.\n",
        "Then for each word in the dictionary, create a probability of that word being in insincere questions and its probability insincere questions. Then finding the conditional probability to use in naive Bayes classifier.\n",
        "Prediction using conditional probabilities.</p>"
      ],
      "metadata": {
        "id": "yUxu_1D_rSq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "6HC79QSOrj_H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "46580928-db5d-48d5-d879-a97833240846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    qid                                      question_text  \\\n",
              "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
              "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
              "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
              "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
              "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
              "\n",
              "   target  \n",
              "0       0  \n",
              "1       0  \n",
              "2       0  \n",
              "3       0  \n",
              "4       0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f60736e0-292e-481d-9e28-f0f49893c7d6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00002165364db923c7e6</td>\n",
              "      <td>How did Quebec nationalists see their province...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000032939017120e6e44</td>\n",
              "      <td>Do you have an adopted dog, how would you enco...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000412ca6e4628ce2cf</td>\n",
              "      <td>Why does velocity affect time? Does velocity a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000042bf85aa498cd78e</td>\n",
              "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000455dfa3e01eae3af</td>\n",
              "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f60736e0-292e-481d-9e28-f0f49893c7d6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f60736e0-292e-481d-9e28-f0f49893c7d6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f60736e0-292e-481d-9e28-f0f49893c7d6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Shape of train ',train.shape)\n",
        "print ('Shape of test ',test.shape)"
      ],
      "metadata": {
        "id": "zjib17VcrusW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b11494-4cb5-49ce-f555-badae1087cc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train  (1306122, 3)\n",
            "Shape of test  (375806, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Taking a look at Sincere Questions')\n",
        "train.loc[train['target'] == 0].sample(5)['question_text']\n",
        "\n",
        "print ('Taking a look at Insincere Questions')\n",
        "train.loc[train['target'] == 1].sample(5)['question_text']\n",
        "#Insincere questions are questions spreading hatred against a group of people or is not real. \n",
        "#An insincere question has a value 1 while a sincere question has target value 0."
      ],
      "metadata": {
        "id": "rBFsuAsvrwia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69d13b19-9a1e-4930-a9ac-4b865b59f7ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taking a look at Sincere Questions\n",
            "Taking a look at Insincere Questions\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "683911     What are home remedies to terminate the pregna...\n",
              "843879     Will middle class Democrats refuse to accept t...\n",
              "1047618        Why are Quorans so obsessed with narcissists?\n",
              "672450     Is the Albanian school system the reason why m...\n",
              "1161209    Is it possible America had something to do wit...\n",
              "Name: question_text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samp = train.sample(1)\n",
        "sentence = samp.iloc[0]['question_text']\n",
        "print (sentence)"
      ],
      "metadata": {
        "id": "J_OuQu-zsI3w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c2ff6f-6f47-472d-ad9e-b90b1461d2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Which acid is found in sweat?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Text Preprocessing in Python\n",
        "import re\n",
        "sentence = re.sub(r'\\d+','',sentence)\n",
        "print ('Sentence After removing numbers\\n',sentence)\n",
        "#Removing Punctuations in a string.\n",
        "import string\n",
        "sentence = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation))\n",
        "print ('Sentence After Removing Punctuations\\n',sentence)"
      ],
      "metadata": {
        "id": "mN0l8t4ssTf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b639628-34a4-4071-fa94-f0c5dd436b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence After removing numbers\n",
            " Which acid is found in sweat?\n",
            "Sentence After Removing Punctuations\n",
            " Which acid is found in sweat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stop Words Stop words are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts.\n",
        "\n",
        "Stop words can be removed by using Natural Language Toolkit (NLTK). NLTK is a set of libraries for symbolic ans statistical natural language processing."
      ],
      "metadata": {
        "id": "svAWCJvbsk10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "words_in_sentence = list(set(sentence.split(' ')) - stop_words)\n",
        "print (words_in_sentence)"
      ],
      "metadata": {
        "id": "vaSDHCX8sTTN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af97924-d32d-4bb7-db8e-230ba885fd79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['God', 'toe', 'put', 'universe', 'Where', 'created', 'big']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "stemmer= PorterStemmer()\n",
        "for i,word in enumerate(words_in_sentence):\n",
        "    words_in_sentence[i] = stemmer.stem(word)\n",
        "print (words_in_sentence)    \n",
        "\n",
        "#Lemmatization of Words\n",
        "#Lemmatisation is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Ex: dogs -> dog. I am not clear with difference between lemmatization and stemming. In most of the tutorials, I found them both and I could not understand the clear difference between the two.\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "words = []\n",
        "for i,word in enumerate(words_in_sentence):\n",
        "    words_in_sentence[i] = lemmatizer.lemmatize(word)\n",
        "print (words_in_sentence)"
      ],
      "metadata": {
        "id": "ySyGw3i0sr90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab491e7c-2602-426a-8305-72624f1b1159"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['god', 'toe', 'put', 'univers', 'where', 'creat', 'big']\n",
            "['god', 'toe', 'put', 'univers', 'where', 'creat', 'big']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Constructing a Naive Bayes Classifier from Scratch.</b>"
      ],
      "metadata": {
        "id": "p7lSOXFssx19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(train, test_size=0.2)"
      ],
      "metadata": {
        "id": "5ez6ccdAsuUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<li>Combine all the preprocessing techniques and create a dictionary of words and each word's count in training data.\n",
        "\n",
        "<li>Calculate probability for each word in a text and filter the words which has probability less than threshold probability. Words with probability less than threshold probability are insignificant.\n",
        "\n",
        "<li>Then for each word in the dictionary, I am creating a probability of that word being in insincere questions and its probability in sincere questions. I am finding the conditional probability to use in naive bayes classifier.\n",
        "\n",
        "<li>Prediction using condtional probabilities."
      ],
      "metadata": {
        "id": "8wGt5dN_s8o1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_count = {}\n",
        "word_count_sincere = {}\n",
        "word_count_insincere = {}\n",
        "sincere  = 0\n",
        "insincere = 0 \n",
        "\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer= PorterStemmer()\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "4xpwZEVzs4ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create three dictionaries which hold word count of words occuring in sincere, insincere and overall after preprocessing each word.\n",
        "row_count = train.shape[0]\n",
        "for row in range(0,row_count):\n",
        "    insincere += train.iloc[row]['target']\n",
        "    sincere += (1 - train.iloc[row]['target'])\n",
        "    sentence = train.iloc[row]['question_text']\n",
        "    sentence = re.sub(r'\\d+','',sentence)\n",
        "    sentence = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation))\n",
        "    words_in_sentence = list(set(sentence.split(' ')) - stop_words)\n",
        "    for index,word in enumerate(words_in_sentence):\n",
        "        word = stemmer.stem(word)\n",
        "        words_in_sentence[index] = lemmatizer.lemmatize(word)\n",
        "    for word in words_in_sentence:\n",
        "        if train.iloc[row]['target'] == 0:   #Sincere Words\n",
        "            if word in word_count_sincere.keys():\n",
        "                word_count_sincere[word]+=1\n",
        "            else:\n",
        "                word_count_sincere[word] = 1\n",
        "        elif train.iloc[row]['target'] == 1: #Insincere Words\n",
        "            if word in word_count_insincere.keys():\n",
        "                word_count_insincere[word]+=1\n",
        "            else:\n",
        "                word_count_insincere[word] = 1\n",
        "        if word in word_count.keys():        #we use this for all words to compute probability.\n",
        "            word_count[word]+=1\n",
        "        else:\n",
        "            word_count[word]=1\n",
        "\n",
        "print('Finish')"
      ],
      "metadata": {
        "id": "tAOM3LwVs40t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10dfc569-408b-476b-c5e9-1ff785118f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding probability for each word in the dictionary. \n",
        "#After that eliminating words which are insignificant. Insignificant words are words which have a probability of occurence less than 0.0001.\n",
        "word_probability = {}\n",
        "total_words = 0\n",
        "for i in word_count:\n",
        "    total_words += word_count[i]\n",
        "for i in word_count:\n",
        "    word_probability[i] = word_count[i] / total_words\n",
        "\n",
        "#Eliminating words which are insignificant. Insignificant words are words which have a probability of occurence less than 0.0001.\n",
        "print ('Total words ',len(word_probability))\n",
        "print ('Minimum probability ',min (word_probability.values()))\n",
        "threshold_p = 0.0001\n",
        "for i in list(word_probability):\n",
        "    if word_probability[i] < threshold_p:\n",
        "        del word_probability[i]\n",
        "        if i in list(word_count_sincere):   #list(dict) return it;s key elements\n",
        "            del word_count_sincere[i]\n",
        "        if i in list(word_count_insincere):  \n",
        "            del word_count_insincere[i]\n",
        "print ('Total words ',len(word_probability))"
      ],
      "metadata": {
        "id": "Keuj0A1itPWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff64ca3e-d3c0-4386-a501-efbe12b9964e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words  143130\n",
            "Minimum probability  1.5856849437470338e-07\n",
            "Total words  1577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding the conditional probability.\n",
        "total_sincere_words = sum(word_count_sincere.values())\n",
        "cp_sincere = {}  #Conditional Probability\n",
        "for i in list(word_count_sincere):\n",
        "    cp_sincere[i] = word_count_sincere[i] / total_sincere_words\n",
        "\n",
        "total_insincere_words = sum(word_count_insincere.values())\n",
        "cp_insincere = {}  #Conditional Probability\n",
        "for i in list(word_count_insincere):\n",
        "    cp_insincere[i] = word_count_insincere[i] / total_insincere_words"
      ],
      "metadata": {
        "id": "oPzksynXtqso"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prediction\n",
        "row_count = test.shape[0]\n",
        "\n",
        "p_insincere = insincere / (sincere + insincere)\n",
        "p_sincere = sincere / (sincere + insincere)\n",
        "accuracy = 0\n",
        "\n",
        "for row in range(0,row_count):\n",
        "    sentence = test.iloc[row]['question_text']\n",
        "    target = test.iloc[row]['target']\n",
        "    sentence = re.sub(r'\\d+','',sentence)\n",
        "    sentence = sentence.translate(sentence.maketrans(\"\",\"\",string.punctuation))\n",
        "    words_in_sentence = list(set(sentence.split(' ')) - stop_words)\n",
        "    for index,word in enumerate(words_in_sentence):\n",
        "        word = stemmer.stem(word)\n",
        "        words_in_sentence[index] = lemmatizer.lemmatize(word)\n",
        "    insincere_term = p_insincere\n",
        "    sincere_term = p_sincere\n",
        "    \n",
        "    sincere_M = len(cp_sincere.keys())\n",
        "    insincere_M = len(cp_insincere.keys())\n",
        "    for word in words_in_sentence:\n",
        "        if word not in cp_insincere.keys():\n",
        "            insincere_M +=1\n",
        "        if word not in cp_sincere.keys():\n",
        "            sincere_M += 1\n",
        "         \n",
        "    for word in words_in_sentence:\n",
        "        if word in cp_insincere.keys():\n",
        "            insincere_term *= (cp_insincere[word] + (1/insincere_M))\n",
        "        else:\n",
        "            insincere_term *= (1/insincere_M)\n",
        "        if word in cp_sincere.keys():\n",
        "            sincere_term *= (cp_sincere[word] + (1/sincere_M))\n",
        "        else:\n",
        "            sincere_term *= (1/sincere_M)\n",
        "        \n",
        "    if insincere_term/(insincere_term + sincere_term) > 0.5:\n",
        "        response = 1\n",
        "    else:\n",
        "        response = 0\n",
        "    if target == response:\n",
        "        accuracy += 1\n",
        "    \n",
        "print ('Accuracy: ',accuracy/row_count*100)"
      ],
      "metadata": {
        "id": "4wDUMgcdtwC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499a248f-8e09-4ba2-da71-c116d65b0a34"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  94.11953296966217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And with this, we come to an end to the notebook. Congratulations, you just learnt the Naive Bayes Algorithm today! We suggest you go through the notebook again to make sure you understood all the concepts thoroughly."
      ],
      "metadata": {
        "id": "LTutheWet8PQ"
      }
    }
  ]
}